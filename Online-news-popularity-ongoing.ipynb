{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article  \n",
    "import csv \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('all') #(Download the package if not downloaded already - Large file) #uncomment it out\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>'Anwar Ka Ajab Kissa' Review: Nawazuddin Siddi...</td>\n",
       "      <td>Anwar Ka Ajab Kissa or Sniffer, written and di...</td>\n",
       "      <td>Anwar Ka Ajab Kissa or Sniffer, written and di...</td>\n",
       "      <td>[movie, ka, anwar, review, dont, siddiqui, kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>COVID-19: Oxford Vaccine To Be Available In In...</td>\n",
       "      <td>And after months of waiting while Coronavirus ...</td>\n",
       "      <td>Pardon my philosophy but the pandemic isn’t ov...</td>\n",
       "      <td>[1000, 2021, indian, today, poonawalla, result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Facebook Messenger Patched A Bug That Would Al...</td>\n",
       "      <td>One of the most widely-known spying methods is...</td>\n",
       "      <td>Fret not, the bug has been patched though, cla...</td>\n",
       "      <td>[snooping, facebook, attacker, messenger, rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Amazon's Making An Upgraded Version Of Its Ech...</td>\n",
       "      <td>Amazon announced today that its experimental s...</td>\n",
       "      <td>Echo Frames have now exited its testing phase,...</td>\n",
       "      <td>[upgraded, smart, echo, devices, volume, assis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NVIDIA GeForce NOW Streaming Service Comes To ...</td>\n",
       "      <td>NVIDIA is using a web-based client to bypass A...</td>\n",
       "      <td>NVIDIA is using a web-based client to bypass A...</td>\n",
       "      <td>[brings, game, google, cloud, geforce, streami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Microsoft Teams Now Offers 24-Hour Video Calls...</td>\n",
       "      <td>Just in time for the holiday season, Microsoft...</td>\n",
       "      <td>Just in time for the holiday season, Microsoft...</td>\n",
       "      <td>[microsoft, zoom, offers, start, calls, 24hour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Netflix For Free In India? All You Need To Kno...</td>\n",
       "      <td>Nothing grabs eyeballs like the word FREE does...</td>\n",
       "      <td>Netflix knows this and it's planning to do som...</td>\n",
       "      <td>[streamfest, need, sign, netflix, streaming, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>‘Godmothered’ Trailer: Jillian Bell Gives Isla...</td>\n",
       "      <td>‘Tis the season to be jolly and it’s raining h...</td>\n",
       "      <td>In fact, if you’ve watched and liked Enchanted...</td>\n",
       "      <td>[gives, bell, fairy, need, tale, eleanor, jone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>'Middle Class Melodies' Review: A Mediocre Mov...</td>\n",
       "      <td>Middle Class Melodies, directed by Vinod Anant...</td>\n",
       "      <td>Now, Middle Class Melodies tries to encapsulat...</td>\n",
       "      <td>[movie, middle, melodies, mediocrity, writing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Akshay Kumar Serves Rs 500 Crore Defamation No...</td>\n",
       "      <td>The Sushant Singh Rajput death case opened a c...</td>\n",
       "      <td>The Sushant Singh Rajput death case opened a c...</td>\n",
       "      <td>[case, youtuber, singh, client, false, kumar, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  'Anwar Ka Ajab Kissa' Review: Nawazuddin Siddi...   \n",
       "1  COVID-19: Oxford Vaccine To Be Available In In...   \n",
       "2  Facebook Messenger Patched A Bug That Would Al...   \n",
       "3  Amazon's Making An Upgraded Version Of Its Ech...   \n",
       "4  NVIDIA GeForce NOW Streaming Service Comes To ...   \n",
       "5  Microsoft Teams Now Offers 24-Hour Video Calls...   \n",
       "6  Netflix For Free In India? All You Need To Kno...   \n",
       "7  ‘Godmothered’ Trailer: Jillian Bell Gives Isla...   \n",
       "8  'Middle Class Melodies' Review: A Mediocre Mov...   \n",
       "9  Akshay Kumar Serves Rs 500 Crore Defamation No...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Anwar Ka Ajab Kissa or Sniffer, written and di...   \n",
       "1  And after months of waiting while Coronavirus ...   \n",
       "2  One of the most widely-known spying methods is...   \n",
       "3  Amazon announced today that its experimental s...   \n",
       "4  NVIDIA is using a web-based client to bypass A...   \n",
       "5  Just in time for the holiday season, Microsoft...   \n",
       "6  Nothing grabs eyeballs like the word FREE does...   \n",
       "7  ‘Tis the season to be jolly and it’s raining h...   \n",
       "8  Middle Class Melodies, directed by Vinod Anant...   \n",
       "9  The Sushant Singh Rajput death case opened a c...   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Anwar Ka Ajab Kissa or Sniffer, written and di...   \n",
       "1  Pardon my philosophy but the pandemic isn’t ov...   \n",
       "2  Fret not, the bug has been patched though, cla...   \n",
       "3  Echo Frames have now exited its testing phase,...   \n",
       "4  NVIDIA is using a web-based client to bypass A...   \n",
       "5  Just in time for the holiday season, Microsoft...   \n",
       "6  Netflix knows this and it's planning to do som...   \n",
       "7  In fact, if you’ve watched and liked Enchanted...   \n",
       "8  Now, Middle Class Melodies tries to encapsulat...   \n",
       "9  The Sushant Singh Rajput death case opened a c...   \n",
       "\n",
       "                                            Keywords  \n",
       "0  [movie, ka, anwar, review, dont, siddiqui, kno...  \n",
       "1  [1000, 2021, indian, today, poonawalla, result...  \n",
       "2  [snooping, facebook, attacker, messenger, rese...  \n",
       "3  [upgraded, smart, echo, devices, volume, assis...  \n",
       "4  [brings, game, google, cloud, geforce, streami...  \n",
       "5  [microsoft, zoom, offers, start, calls, 24hour...  \n",
       "6  [streamfest, need, sign, netflix, streaming, k...  \n",
       "7  [gives, bell, fairy, need, tale, eleanor, jone...  \n",
       "8  [movie, middle, melodies, mediocrity, writing,...  \n",
       "9  [case, youtuber, singh, client, false, kumar, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#crawling using beautiful soup\n",
    "\n",
    "url=\"https://in.mashable.com/\"\n",
    "r=requests.get(url)\n",
    "soup=BeautifulSoup(r.content,'html5lib') \n",
    "table=soup.findAll('a',attrs={'class':'title'})\n",
    "news=[]\n",
    "for row in table: \n",
    "    if not row['href'].startswith('http'):\n",
    "        news.append('https://in.mashable.com'+row['href'])\n",
    "\n",
    "df=[]\n",
    "for i in news:\n",
    "    article = Article(i, language=\"en\")\n",
    "    article.download() \n",
    "    article.parse() \n",
    "    article.nlp() \n",
    "    data={}\n",
    "    data['Title']=article.title\n",
    "    data['Text']=article.text\n",
    "    data['Summary']=article.summary\n",
    "    data['Keywords']=article.keywords\n",
    "    df.append(data)\n",
    "\n",
    "    \n",
    "dataset=pd.DataFrame(df)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>http://mashable.com/2013/01/07/beewi-smart-toys/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0.559889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.698198</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.195000</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>http://mashable.com/2013/01/07/bodymedia-armba...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>0.418163</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.549834</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.224479</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>http://mashable.com/2013/01/07/canon-poweshot-n/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>989.0</td>\n",
       "      <td>0.433574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.572108</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.242778</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>http://mashable.com/2013/01/07/car-of-the-futu...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>http://mashable.com/2013/01/07/chuck-hagel-web...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.238095</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...       731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...       731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...       731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/       731.0   \n",
       "5   http://mashable.com/2013/01/07/beewi-smart-toys/       731.0   \n",
       "6  http://mashable.com/2013/01/07/bodymedia-armba...       731.0   \n",
       "7   http://mashable.com/2013/01/07/canon-poweshot-n/       731.0   \n",
       "8  http://mashable.com/2013/01/07/car-of-the-futu...       731.0   \n",
       "9  http://mashable.com/2013/01/07/chuck-hagel-web...       731.0   \n",
       "\n",
       "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
       "0             12.0              219.0          0.663594                1.0   \n",
       "1              9.0              255.0          0.604743                1.0   \n",
       "2              9.0              211.0          0.575130                1.0   \n",
       "3              9.0              531.0          0.503788                1.0   \n",
       "4             13.0             1072.0          0.415646                1.0   \n",
       "5             10.0              370.0          0.559889                1.0   \n",
       "6              8.0              960.0          0.418163                1.0   \n",
       "7             12.0              989.0          0.433574                1.0   \n",
       "8             11.0               97.0          0.670103                1.0   \n",
       "9             10.0              231.0          0.636364                1.0   \n",
       "\n",
       "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  ...  \\\n",
       "0                   0.815385         4.0              2.0        1.0  ...   \n",
       "1                   0.791946         3.0              1.0        1.0  ...   \n",
       "2                   0.663866         3.0              1.0        1.0  ...   \n",
       "3                   0.665635         9.0              0.0        1.0  ...   \n",
       "4                   0.540890        19.0             19.0       20.0  ...   \n",
       "5                   0.698198         2.0              2.0        0.0  ...   \n",
       "6                   0.549834        21.0             20.0       20.0  ...   \n",
       "7                   0.572108        20.0             20.0       20.0  ...   \n",
       "8                   0.836735         2.0              0.0        0.0  ...   \n",
       "9                   0.797101         4.0              1.0        1.0  ...   \n",
       "\n",
       "    min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
       "0                0.100000                     0.7               -0.350000   \n",
       "1                0.033333                     0.7               -0.118750   \n",
       "2                0.100000                     1.0               -0.466667   \n",
       "3                0.136364                     0.8               -0.369697   \n",
       "4                0.033333                     1.0               -0.220192   \n",
       "5                0.136364                     0.6               -0.195000   \n",
       "6                0.100000                     1.0               -0.224479   \n",
       "7                0.100000                     1.0               -0.242778   \n",
       "8                0.400000                     0.8               -0.125000   \n",
       "9                0.100000                     0.5               -0.238095   \n",
       "\n",
       "    min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
       "0                  -0.600               -0.200000             0.500000   \n",
       "1                  -0.125               -0.100000             0.000000   \n",
       "2                  -0.800               -0.133333             0.000000   \n",
       "3                  -0.600               -0.166667             0.000000   \n",
       "4                  -0.500               -0.050000             0.454545   \n",
       "5                  -0.400               -0.100000             0.642857   \n",
       "6                  -0.500               -0.050000             0.000000   \n",
       "7                  -0.500               -0.050000             1.000000   \n",
       "8                  -0.125               -0.125000             0.125000   \n",
       "9                  -0.500               -0.100000             0.000000   \n",
       "\n",
       "    title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "0                  -0.187500                 0.000000   \n",
       "1                   0.000000                 0.500000   \n",
       "2                   0.000000                 0.500000   \n",
       "3                   0.000000                 0.500000   \n",
       "4                   0.136364                 0.045455   \n",
       "5                   0.214286                 0.142857   \n",
       "6                   0.000000                 0.500000   \n",
       "7                   0.500000                 0.500000   \n",
       "8                   0.000000                 0.375000   \n",
       "9                   0.000000                 0.500000   \n",
       "\n",
       "    abs_title_sentiment_polarity   shares  \n",
       "0                       0.187500      593  \n",
       "1                       0.000000      711  \n",
       "2                       0.000000     1500  \n",
       "3                       0.000000     1200  \n",
       "4                       0.136364      505  \n",
       "5                       0.214286      855  \n",
       "6                       0.000000      556  \n",
       "7                       0.500000      891  \n",
       "8                       0.000000     3600  \n",
       "9                       0.000000      710  \n",
       "\n",
       "[10 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILEPATH=\"OnlineNewsPopularity.csv\" #dataset from UCI Repo\n",
    "data1=pd.read_csv(FILEPATH)\n",
    "data1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cols={x: x.lower().strip() for x in list(data1)}\n",
    "data1=data1.rename(index=str,columns=clean_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(data1, test_size=0.4, random_state=42)\n",
    "\n",
    "drop_attributes_list = ['url','shares', 'timedelta','num_self_hrefs', 'kw_min_min', 'kw_max_min', 'kw_avg_min','kw_min_max','kw_max_max','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_max_shares','self_reference_avg_sharess']\n",
    "\n",
    "x_train = train_set.drop(drop_attributes_list, axis=1)\n",
    "y_train = train_set['shares']\n",
    "\n",
    "x_test = test_set.drop(drop_attributes_list, axis=1)\n",
    "y_test = test_set['shares']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Trained successful\n"
     ]
    }
   ],
   "source": [
    "print(\"Training classifier...\")\n",
    "clf=RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "clf.fit(x_train,y_train)\n",
    "print(\"Trained successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual shares</th>\n",
       "      <th>Predicted shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1800</td>\n",
       "      <td>1680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>790</td>\n",
       "      <td>841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>872</td>\n",
       "      <td>884.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2500</td>\n",
       "      <td>2383.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>860</td>\n",
       "      <td>1750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1000</td>\n",
       "      <td>1047.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>47300</td>\n",
       "      <td>25569.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>20400</td>\n",
       "      <td>16584.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2500</td>\n",
       "      <td>2470.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual shares  Predicted shares\n",
       "0           1900            2200.0\n",
       "1           1800            1680.0\n",
       "2            790             841.0\n",
       "3            872             884.0\n",
       "4           2500            2383.6\n",
       "5            860            1750.0\n",
       "6           1000            1047.8\n",
       "7          47300           25569.6\n",
       "8          20400           16584.5\n",
       "9           2500            2470.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result=pd.DataFrame(clf.predict(x_train),list(y_train))\n",
    "result.reset_index(level=0, inplace=True)\n",
    "result_df = result.rename(index=str, columns={\"index\": \"Actual shares\", 0: \"Predicted shares\"})\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of headlines in the dataset: 1186018\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>ambitious olsson wins triple jump</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>antic delighted with record breaking barca</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>aussie qualifier stosur wastes four memphis match</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>aust addresses un security council over iraq</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>australia is locked into war timetable opp</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4\n",
       "5                  ambitious olsson wins triple jump      5\n",
       "6         antic delighted with record breaking barca      6\n",
       "7  aussie qualifier stosur wastes four memphis match      7\n",
       "8       aust addresses un security council over iraq      8\n",
       "9         australia is locked into war timetable opp      9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILEPATH2=\"abcnews-date-text.csv\"\n",
    "data = pd.read_csv(FILEPATH2, error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "print(\"Number of headlines in the dataset:\", len(documents))\n",
    "# print(len(documents))\n",
    "# print(documents[:5])\n",
    "documents.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "#pip install gensim (install gensim packages if you havent installed it already)\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing the text of the article - breaking into base words follwed by stemming \n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "#Removing the stopword and considering only token length > 3 \n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['a', 'g', 'calls', 'for', 'infrastructure', 'protection', 'summit']\n",
      "\n",
      "\n",
      "tokenized and lemmatized document: \n",
      "['call', 'infrastructur', 'protect', 'summit']\n"
     ]
    }
   ],
   "source": [
    "#example of how stemming and lemmatizing is being done by taking an example from the dataset. \n",
    "doc_sample = documents[documents['index'] ==2].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n')\n",
    "print('tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [decid, commun, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(67259 unique tokens: ['broadcast', 'commun', 'decid', 'licenc', 'awar']...)\n",
      "0 broadcast\n",
      "1 commun\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n",
      "11 aust\n",
      "12 rise\n",
      "13 staff\n",
      "14 strike\n",
      "15 affect\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "print(dictionary)\n",
    "count=0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 1), (8, 1), (9, 1), (10, 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc2bow is bag of words. Assgin the count or freq of the tokens(given by unique id in dictionary)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 7 i.e. (\"call\") appears 1 time.\n",
      "Word 8 i.e. (\"infrastructur\") appears 1 time.\n",
      "Word 9 i.e. (\"protect\") appears 1 time.\n",
      "Word 10 i.e. (\"summit\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "#printing the example of doc2bow\n",
    "bow_doc_2 = bow_corpus[2]\n",
    "for i in range(len(bow_doc_2)):\n",
    "    print(\"Word {} i.e. (\\\"{}\\\") appears {} time.\".format(bow_doc_2[i][0], \n",
    "                                               dictionary[bow_doc_2[i][0]], \n",
    "bow_doc_2[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.010*\"plan\" + 0.009*\"govern\" + 0.008*\"rural\" + 0.008*\"council\" + 0.008*\"interview\" + 0.008*\"farmer\" + 0.008*\"water\" + 0.007*\"countri\" + 0.007*\"fund\" + 0.007*\"queensland\"\n",
      "Topic: 1 \n",
      "Words: 0.028*\"polic\" + 0.013*\"death\" + 0.011*\"woman\" + 0.010*\"crash\" + 0.009*\"news\" + 0.009*\"south\" + 0.009*\"donald\" + 0.009*\"kill\" + 0.009*\"coast\" + 0.009*\"attack\"\n",
      "Topic: 2 \n",
      "Words: 0.019*\"australia\" + 0.016*\"trump\" + 0.015*\"year\" + 0.010*\"market\" + 0.010*\"world\" + 0.008*\"die\" + 0.008*\"live\" + 0.007*\"final\" + 0.006*\"test\" + 0.006*\"say\"\n",
      "Topic: 3 \n",
      "Words: 0.024*\"australian\" + 0.009*\"shoot\" + 0.008*\"say\" + 0.008*\"australia\" + 0.008*\"tasmania\" + 0.007*\"time\" + 0.007*\"royal\" + 0.007*\"elect\" + 0.005*\"drum\" + 0.005*\"open\"\n",
      "Topic: 4 \n",
      "Words: 0.017*\"charg\" + 0.015*\"court\" + 0.013*\"murder\" + 0.010*\"face\" + 0.009*\"health\" + 0.009*\"accus\" + 0.007*\"trial\" + 0.007*\"alleg\" + 0.007*\"child\" + 0.006*\"commun\"\n"
     ]
    }
   ],
   "source": [
    "#map the entire corpus into num_topics = 5 topics\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    \n",
    "#It has picked up most the most contributing words/tokens with probabilty \n",
    "#Probabilty (which represent the contribution of that word towards the topic or corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8381300568580627\t \n",
      "Topic: 0.010*\"plan\" + 0.009*\"govern\" + 0.008*\"rural\" + 0.008*\"council\" + 0.008*\"interview\"\n",
      "\n",
      "Score: 0.040902089327573776\t \n",
      "Topic: 0.024*\"australian\" + 0.009*\"shoot\" + 0.008*\"say\" + 0.008*\"australia\" + 0.008*\"tasmania\"\n",
      "\n",
      "Score: 0.040536362677812576\t \n",
      "Topic: 0.019*\"australia\" + 0.016*\"trump\" + 0.015*\"year\" + 0.010*\"market\" + 0.010*\"world\"\n",
      "\n",
      "Score: 0.04042215272784233\t \n",
      "Topic: 0.017*\"charg\" + 0.015*\"court\" + 0.013*\"murder\" + 0.010*\"face\" + 0.009*\"health\"\n",
      "\n",
      "Score: 0.040009383112192154\t \n",
      "Topic: 0.028*\"polic\" + 0.013*\"death\" + 0.011*\"woman\" + 0.010*\"crash\" + 0.009*\"news\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_LDA(text):\n",
    "    unseen_document = text\n",
    "    bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "    result=[]\n",
    "    for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "        #print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n",
    "        result.append(score)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize #extract the tokens(words) from string of characters\n",
    "from nltk.corpus import stopwords #Stopwords are the English words which does not add much meaning to a sentence.\n",
    "stopwords=set(stopwords.words('english'))\n",
    "\n",
    "from textblob import TextBlob #library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more\n",
    "import datefinder\n",
    "import datetime  \n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes functions\n",
    "# n_unique_tokens: Rate of unique words in the content\n",
    "def for_unique_tokens_rate(words):\n",
    "    words=tokenize(words)\n",
    "    no_order = list(set(words))\n",
    "    unique_tokens_rate=len(no_order)/len(words)\n",
    "    return unique_tokens_rate\n",
    "\n",
    "# n_non_stop_words: Rate of non-stop words in the content\n",
    "def for_nonstop_words_rate(words):\n",
    "    words=tokenize(words)\n",
    "    filtered_sentence = [w for w in words if not w in stopwords]\n",
    "    nonstop_words_rate=len(filtered_sentence)/len(words)\n",
    "    return nonstop_words_rate\n",
    "\n",
    "# n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "def for_unique_nonstop_words_rate(words):\n",
    "    words=tokenize(words)\n",
    "    filtered_sentence = [w for w in words if not w in stopwords]\n",
    "    no_order = list(set(filtered_sentence))\n",
    "    unique_nonstop_words_rate=len(no_order)/len(words)\n",
    "    return unique_nonstop_words_rate\n",
    "\n",
    "# average_token_length: Average length of the words in the content\n",
    "def avg_token_length(words):\n",
    "    words=tokenize(words)\n",
    "    length=[]\n",
    "    for i in words:\n",
    "        length.append(len(i))\n",
    "    return np.average(length)\n",
    "\n",
    "def day(article_text):\n",
    "    article=article_text\n",
    "    if len(list(datefinder.find_dates(article_text)))>0:\n",
    "        date=str(list(datefinder.find_dates(article_text))[0])\n",
    "        date=date.split()\n",
    "        date=date[0]\n",
    "        year, month, day = date.split('-')     \n",
    "        day_name = datetime.date(int(year), int(month), int(day)) \n",
    "        return day_name.strftime(\"%A\")\n",
    "    return \"Monday\" #else return default day as monday\n",
    "\n",
    "def tokenize(text):\n",
    "    text=text\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Non-neutral tokens are those tokens which are more likely to exhibit the following POS Tags: Nouns, Adjectives, Adverbs, Interjections\n",
    "def non_neutral_tokens(text):\n",
    "    processed_docs = preprocess(text)\n",
    "    speech=nltk.pos_tag(processed_docs)\n",
    "    tags=[\"NN\",\"NNP\",\"NNS\",\"JJ\",\"JJR\",\"JJS\",\"RB\",\"RBR\",\"RBS\",\"UH\"]\n",
    "    speech_list=[i[0] for i in speech if i[1] in tags]\n",
    "    return len(speech_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words=[]\n",
    "negative_words=[]\n",
    "def polar(words):\n",
    "    all_tokens=tokenize(words)\n",
    "    for i in all_tokens:\n",
    "        analysis=TextBlob(i)\n",
    "        polarity=analysis.sentiment.polarity\n",
    "        if polarity>0:\n",
    "            positive_words.append(i)\n",
    "        if polarity<0:\n",
    "            negative_words.append(i)\n",
    "    return positive_words,negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_rate_positive_words: Rate of positive words in the content\n",
    "# global_rate_negative_words: Rate of negative words in the content\n",
    "# rate_positive_words: Rate of positive words among non-neutral tokens\n",
    "# rate_negative_words: Rate of negative words among non-neutral tokens\n",
    "\n",
    "def rates(words):\n",
    "    words=polar(words)\n",
    "    positive=words[0]\n",
    "    negative=words[1]\n",
    "    \n",
    "    global_rate_positive_words=(len(positive)/len(words))/100\n",
    "    global_rate_negative_words=(len(negative)/len(words))/100\n",
    "    \n",
    "    polar_positive=[]\n",
    "    polar_negative=[]\n",
    "    \n",
    "    for i in positive:\n",
    "        analysis=TextBlob(i)\n",
    "        polar_positive.append(analysis.sentiment.polarity)\n",
    "        avg_positive_polarity=analysis.sentiment.polarity # avg_positive_polarity: Avg. polarity of positive words\n",
    "    for j in negative:\n",
    "        analysis2=TextBlob(j)\n",
    "        polar_negative.append(analysis2.sentiment.polarity)\n",
    "        avg_negative_polarity=analysis2.sentiment.polarity # avg_negative_polarity: Avg. polarity of negative words\n",
    "        \n",
    "    min_positive_polarity=min(polar_positive)\n",
    "    max_positive_polarity=max(polar_positive)\n",
    "    min_negative_polarity=min(polar_negative)\n",
    "    max_negative_polarity=max(polar_negative)\n",
    "    avg_positive_polarity=np.average(polar_positive)\n",
    "    avg_negative_polarity=np.average(polar_negative)\n",
    "    return global_rate_positive_words,global_rate_negative_words,avg_positive_polarity,min_positive_polarity,max_positive_polarity,avg_negative_polarity,min_negative_polarity,max_negative_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=[]\n",
    "for i in news:\n",
    "    pred_info={}\n",
    "    article = Article(i, language=\"en\") # en for English \n",
    "    article.download() \n",
    "    article.parse()\n",
    "    analysis=TextBlob(article.text)\n",
    "    polarity=analysis.sentiment.polarity\n",
    "    title_analysis=TextBlob(article.title)\n",
    "    pred_info['text']=article.text\n",
    "    pred_info['n_tokens_title']=len(tokenize(article.title))\n",
    "    pred_info['n_tokens_content']=len(tokenize(article.text))\n",
    "    pred_info['n_unique_tokens']=for_unique_tokens_rate(article.text)\n",
    "    pred_info['n_non_stop_words']=for_nonstop_words_rate(article.text)\n",
    "    pred_info['n_non_stop_unique_tokens']=for_unique_nonstop_words_rate(article.text)\n",
    "    pred_info['num_hrefs']=article.html.count(\"https://in.mashable.com\")\n",
    "    pred_info['num_imgs']=len(article.images)\n",
    "    pred_info['num_videos']=len(article.movies)\n",
    "    pred_info['average_token_length']=avg_token_length(article.text)\n",
    "    pred_info['num_keywords']=len(article.keywords)\n",
    "    \n",
    "    \n",
    "    # categorizing  of news article according to lifestyle, entertainment, business etc\n",
    "    if \"life-style\" in article.url: \n",
    "        pred_info['data_channel_is_lifestyle']=1\n",
    "    else:\n",
    "        pred_info['data_channel_is_lifestyle']=0\n",
    "    if \"etimes\" in article.url:\n",
    "        pred_info['data_channel_is_entertainment']=1\n",
    "    else:\n",
    "        pred_info['data_channel_is_entertainment']=0\n",
    "    if \"business\" in article.url:\n",
    "        pred_info['data_channel_is_bus']=1\n",
    "    else:\n",
    "        pred_info['data_channel_is_bus']=0\n",
    "    if \"social media\" or \"facebook\" or \"whatsapp\" in article.text.lower():\n",
    "        data_channel_is_socmed=1\n",
    "        data_channel_is_tech=0\n",
    "        data_channel_is_world=0\n",
    "    else:\n",
    "        data_channel_is_socmed=0\n",
    "    if (\"technology\" or \"tech\" in article.text.lower()) or (\"technology\" or \"tech\" in article.url):\n",
    "        data_channel_is_tech=1\n",
    "        data_channel_is_socmed=0\n",
    "        data_channel_is_world=0\n",
    "    else:\n",
    "        data_channel_is_tech=0\n",
    "    if \"world\" in article.url:\n",
    "        data_channel_is_world=1\n",
    "        data_channel_is_tech=0\n",
    "        data_channel_is_socmed=0\n",
    "    else:\n",
    "        data_channel_is_world=0\n",
    "        \n",
    "    pred_info['data_channel_is_socmed']=data_channel_is_socmed\n",
    "    pred_info['data_channel_is_tech']=data_channel_is_tech\n",
    "    pred_info['data_channel_is_world']=data_channel_is_world\n",
    "    \n",
    "    if day(i)==\"Monday\":\n",
    "        pred_info['weekday_is_monday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_monday']=0\n",
    "    if day(i)==\"Tuesday\":\n",
    "        pred_info['weekday_is_tuesday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_tuesday']=0\n",
    "    if day(i)==\"Wednesday\":\n",
    "        pred_info['weekday_is_wednesday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_wednesday']=0\n",
    "    if day(i)==\"Thursday\":\n",
    "        pred_info['weekday_is_thursday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_thursday']=0\n",
    "    if day(i)==\"Friday\":\n",
    "        pred_info['weekday_is_friday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_friday']=0\n",
    "    if day(i)==\"Saturday\":\n",
    "        pred_info['weekday_is_saturday']=1\n",
    "        pred_info['is_weekend']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_saturday']=0\n",
    "    if day(i)==\"Sunday\":\n",
    "        pred_info['weekday_is_sunday']=1\n",
    "        pred_info['is_weekend']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_sunday']=0\n",
    "        pred_info['is_weekend']=0\n",
    "    \n",
    "    #   LDA_00: Closeness to LDA topic 0\n",
    "    #   LDA_01: Closeness to LDA topic 1\n",
    "    #   LDA_02: Closeness to LDA topic 2\n",
    "    #   LDA_03: Closeness to LDA topic 3\n",
    "    #   LDA_04: Closeness to LDA topic 4\n",
    "    #   we have consider 5 topics    \n",
    "    pred_info['LDA_00']=get_LDA(article.text)[0]\n",
    "    pred_info['LDA_01']=get_LDA(article.text)[1]\n",
    "    pred_info['LDA_02']=get_LDA(article.text)[2]\n",
    "    pred_info['LDA_03']=get_LDA(article.text)[3]\n",
    "    pred_info['LDA_04']=get_LDA(article.text)[4]\n",
    "    \n",
    "    \n",
    "    pred_info['global_subjectivity']=analysis.sentiment.subjectivity\n",
    "    pred_info['global_sentiment_polarity']=analysis.sentiment.polarity\n",
    "    pred_info['global_rate_positive_words']=rates(article.text)[0]\n",
    "    pred_info['global_rate_negative_words']=rates(article.text)[1]\n",
    "    pred_info['avg_positive_polarity']=rates(article.text)[2]\n",
    "    pred_info['min_positive_polarity']=rates(article.text)[3]\n",
    "    pred_info['max_positive_polarity']=rates(article.text)[4]\n",
    "    pred_info['avg_negative_polarity']=rates(article.text)[5]\n",
    "    pred_info['min_negative_polarity']=rates(article.text)[6]\n",
    "    pred_info['max_negative_polarity']=rates(article.text)[7]\n",
    "    pred_info['rate_positive_words']=len(polar(article.text)[0])/non_neutral_tokens(article.text)\n",
    "    pred_info['rate_negative_words']=len(polar(article.text)[1])/non_neutral_tokens(article.text)\n",
    "    pred_info['title_subjectivity']=title_analysis.sentiment.subjectivity\n",
    "    pred_info['title_sentiment_polarity']=title_analysis.sentiment.polarity\n",
    "    pred_info['abs_title_subjectivity']=abs(title_analysis.sentiment.subjectivity-0.5)\n",
    "    pred_info['abs_title_sentiment_polarity']=abs(title_analysis.sentiment.polarity)\n",
    "    df1.append(pred_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1723</td>\n",
       "      <td>0.329658</td>\n",
       "      <td>0.575160</td>\n",
       "      <td>0.272200</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3.688915</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.322988</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.746637</td>\n",
       "      <td>0.627803</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>541</td>\n",
       "      <td>0.545287</td>\n",
       "      <td>0.619224</td>\n",
       "      <td>0.430684</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4.349353</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.319634</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2.988372</td>\n",
       "      <td>1.918605</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>355</td>\n",
       "      <td>0.529577</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.490141</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.300009</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>5.990476</td>\n",
       "      <td>3.619048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>530</td>\n",
       "      <td>0.556604</td>\n",
       "      <td>0.667925</td>\n",
       "      <td>0.462264</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4.422642</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.288286</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>4.162437</td>\n",
       "      <td>2.335025</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>412</td>\n",
       "      <td>0.558252</td>\n",
       "      <td>0.691748</td>\n",
       "      <td>0.432039</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4.131068</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.303337</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>6.101351</td>\n",
       "      <td>3.918919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0              19              1723         0.329658          0.575160   \n",
       "1              17               541         0.545287          0.619224   \n",
       "2              14               355         0.529577          0.633803   \n",
       "3              13               530         0.556604          0.667925   \n",
       "4              13               412         0.558252          0.691748   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_imgs  num_videos  \\\n",
       "0                  0.272200         33        13           0   \n",
       "1                  0.430684         33         9           0   \n",
       "2                  0.422535         33         8           0   \n",
       "3                  0.462264         33        10           0   \n",
       "4                  0.432039         33        10           0   \n",
       "\n",
       "   average_token_length  num_keywords  ...  max_positive_polarity  \\\n",
       "0              3.688915             0  ...                    1.0   \n",
       "1              4.349353             0  ...                    1.0   \n",
       "2              4.490141             0  ...                    1.0   \n",
       "3              4.422642             0  ...                    1.0   \n",
       "4              4.131068             0  ...                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.322988                   -0.7                  -0.05   \n",
       "1              -0.319634                   -0.7                  -0.05   \n",
       "2              -0.300009                   -0.7                  -0.05   \n",
       "3              -0.288286                   -0.7                  -0.05   \n",
       "4              -0.303337                   -1.0                  -0.05   \n",
       "\n",
       "   rate_positive_words  rate_negative_words  title_subjectivity  \\\n",
       "0             0.746637             0.627803            0.466667   \n",
       "1             2.988372             1.918605            0.400000   \n",
       "2             5.990476             3.619048            0.000000   \n",
       "3             4.162437             2.335025            0.400000   \n",
       "4             6.101351             3.918919            0.000000   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                      -0.3                0.033333   \n",
       "1                       0.4                0.100000   \n",
       "2                       0.0                0.500000   \n",
       "3                       0.4                0.100000   \n",
       "4                       0.0                0.500000   \n",
       "\n",
       "   abs_title_sentiment_polarity  \n",
       "0                           0.3  \n",
       "1                           0.4  \n",
       "2                           0.0  \n",
       "3                           0.4  \n",
       "4                           0.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df=pd.DataFrame(df1)\n",
    "pred_test=pred_df.drop(['text'],axis=1)\n",
    "pred_df.head()\n",
    "# pred_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Virality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Anwar Ka Ajab Kissa or Sniffer, written and di...</td>\n",
       "      <td>22643.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>And after months of waiting while Coronavirus ...</td>\n",
       "      <td>14517.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>One of the most widely-known spying methods is...</td>\n",
       "      <td>22943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Amazon announced today that its experimental s...</td>\n",
       "      <td>13048.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NVIDIA is using a web-based client to bypass A...</td>\n",
       "      <td>12938.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Just in time for the holiday season, Microsoft...</td>\n",
       "      <td>16218.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Nothing grabs eyeballs like the word FREE does...</td>\n",
       "      <td>17309.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>‘Tis the season to be jolly and it’s raining h...</td>\n",
       "      <td>7182.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Middle Class Melodies, directed by Vinod Anant...</td>\n",
       "      <td>17613.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>The Sushant Singh Rajput death case opened a c...</td>\n",
       "      <td>14724.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>'BE' (Deluxe Edition) is officially out and BT...</td>\n",
       "      <td>23231.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>It was only a matter of time before iFixit got...</td>\n",
       "      <td>20088.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>And Priyanka Chopra Jonas is back on our scree...</td>\n",
       "      <td>12484.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Those following President Trump on Twitter hav...</td>\n",
       "      <td>20790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Former New York City mayor and current Trump c...</td>\n",
       "      <td>26530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>We've made it to the homestretch of 2020, and ...</td>\n",
       "      <td>11069.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>I had a long standing dream to dance to Chunar...</td>\n",
       "      <td>34490.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  Virality\n",
       "0   Anwar Ka Ajab Kissa or Sniffer, written and di...   22643.7\n",
       "1   And after months of waiting while Coronavirus ...   14517.2\n",
       "2   One of the most widely-known spying methods is...   22943.0\n",
       "3   Amazon announced today that its experimental s...   13048.8\n",
       "4   NVIDIA is using a web-based client to bypass A...   12938.9\n",
       "5   Just in time for the holiday season, Microsoft...   16218.8\n",
       "6   Nothing grabs eyeballs like the word FREE does...   17309.4\n",
       "7   ‘Tis the season to be jolly and it’s raining h...    7182.5\n",
       "8   Middle Class Melodies, directed by Vinod Anant...   17613.7\n",
       "9   The Sushant Singh Rajput death case opened a c...   14724.6\n",
       "10  'BE' (Deluxe Edition) is officially out and BT...   23231.1\n",
       "11  It was only a matter of time before iFixit got...   20088.6\n",
       "12  And Priyanka Chopra Jonas is back on our scree...   12484.6\n",
       "13  Those following President Trump on Twitter hav...   20790.0\n",
       "14  Former New York City mayor and current Trump c...   26530.0\n",
       "15  We've made it to the homestretch of 2020, and ...   11069.0\n",
       "16  I had a long standing dream to dance to Chunar...   34490.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2=pd.DataFrame(clf.predict(pred_test),pred_df['text'])\n",
    "test2.reset_index(level=0, inplace=True)\n",
    "test2 = test2.rename(index=str, columns={\"index\": \"News\", 0: \"Virality\"})\n",
    "test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
